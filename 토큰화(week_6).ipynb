{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "토큰화(week 6).ipynb",
      "provenance": [],
      "mount_file_id": "1KMSG7LgrEqLh5G0-8nhMiPDd44sdzTO7",
      "authorship_tag": "ABX9TyPOQmfkSwzoOPzsFjC9ezq/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dianakang/K_digital_likelion/blob/master/%ED%86%A0%ED%81%B0%ED%99%94(week_6).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URviZ5CuUMyn"
      },
      "source": [
        "# 1.1.1 토크나이징"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agGXbtT3UQkO"
      },
      "source": [
        "# 우리가 일상에서 사용하는 언어를 '자연어'라고 한다.\r\n",
        "# 컴퓨터는 자연어를 직접 이해할 수 없다. 따라서 컴퓨터가 이해할 수 있도록 처리해줘야 하는데, 그것을 자연어처리(Natural Language Processing, NLP)라고 한다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytwwTppxUjQw"
      },
      "source": [
        "# 그럼 어떻게 해야 컴퓨터에게 자연어를 이해시킬 수 있을까?\r\n",
        "# 우선 어떤 문장을 일정한 의미가 있는 가장 작은 단어들로 나눈다. 그 다음 나눠진 단어들을 이용해 의미를 분석한다.\r\n",
        "# 여기서 가장 기본이 되는 단어들을 '토큰(token)'이라고 한다. \r\n",
        "# 그리고 문장에서 토큰 단위로 정보를 나누는 작업을 '토크나이징'이라고 한다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5gFTQP0hW7-"
      },
      "source": [
        "# 1.1.2 KoNLPy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NblXLuJHVXE0"
      },
      "source": [
        "# 토크나이징은 문장 형태의 데이터를 처리하기 위해 제일 처음 수행해야 하는 전처리 작업이다.\r\n",
        "# 한국어 토크나이징을 지원하는 파이썬 모듈(라이브러리)에는 KoNLPy(코엔엘파이)가 있다. 오픈소스 소프트웨어이다. \r\n",
        "# 한국어 자연어 처리를 할 때 토큰 단위는 '형태소'이다.\r\n",
        "# '형태소'란 일정한 의미가 있는 가장 작은 말의 단위이다. 즉, 의미가 더 이상 쪼개지지 않는 단어를 뜻한다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHaSnKVfWT_K"
      },
      "source": [
        "# 그럼 문장을 어떻게 형태소 단위로 토크나이징 할 수 있을까?\r\n",
        "# 영어의 경우 단어의 변화가 크지 않고, 띄어쓰기로 단어를 구분하기 때문에 공백을 기준으로 토크나이징을 수행해도 큰 문제가 없다.\r\n",
        "# 하지만 한국어의 경우에는 언어의 특성상 띄어쓰기만으로 토크나이징을 하는 것이 어렵다.\r\n",
        "# 따라서 한국어 문장에서 형태소를 분석할 수 있는 도구가 필요한데, 이를 '형태소 분석기'라고 한다.\r\n",
        "# 이는 문장에서 형태소뿐만 아니라 어근, 접두사/접미사, 품사 등 다양한 언어적 속성의 구조를 파악하게 해주며, 형태소의 듯과 문맥을 고려해 품사 태깅을 해준다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKhESQBWhh5T"
      },
      "source": [
        "# 그럼 이제 KoNLPy을 설치한 후 사용하기 좋은 세 가지 형태소 분석기 모듈을 살펴보자."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jaXfAXHHYQn",
        "outputId": "7ac14624-f30a-41d5-e8f1-be346c78826f"
      },
      "source": [
        "pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.3MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/af/93f92b38ec1ff3091cd38982ed19cea2800fefb609b5801c41fc43c0781e/JPype1-1.2.1-cp36-cp36m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 69.3MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/67/c3/6bed87f3b1e5ed2f34bd58bf7978e308c86e255193916be76e5a5ce5dfca/tweepy-3.10.0-py2.py3-none-any.whl\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, beautifulsoup4, tweepy, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjaqybR4h51x"
      },
      "source": [
        "## (1) Kkma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrdGWgrmh-EM"
      },
      "source": [
        "# Kkma는 '꼬꼬마'라고 불린다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3t2nJYbwiiy"
      },
      "source": [
        "from konlpy.tag import Kkma\r\n",
        "# konlpy.tag 패키지의 Kkma 모듈을 불러온다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiRcqZoYiZ_6"
      },
      "source": [
        "# Kkma에서 사용하는 4가지 함수\r\n",
        "## morphs(phrase) -> 형태소 단위로 토크나이징 / 리스트 형태로 반환\r\n",
        "## nouns(phrase) -> 품사가 명사인 토큰만 추출\r\n",
        "## pos(phrase,flatten=True) -> 형태소 추출 후 품사 태깅 / 튜플{}형태로 묶여서 리스트[]로 반환\r\n",
        "## sentences(phrase) -> 문장 분리 / 리스트 형태로 반환"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM5phfHqEhvU"
      },
      "source": [
        "# 꼬꼬마 형태소 분석기 객체 생성\r\n",
        "kkma = Kkma()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Tm0wjD8jDH-"
      },
      "source": [
        "text = \"인공지능 챗봇 이루다는 연애의 과학에서 학습했습니다.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N3pTnzaHgzx",
        "outputId": "07db262c-aa2a-4d4a-8262-647e05d92e61"
      },
      "source": [
        "# 형태소 추출\r\n",
        "morphs = kkma.morphs(text)  ## text변수에 저장된 문장을 형태소 단위로 토크나이징 \r\n",
        "print(morphs)   ## 토크나이징된 형태소들은 리스트 형태로 반환"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['인공지능', '챗봇', '이루', '달', '는', '연애', '의', '과학', '에서', '학습', '하', '었', '습니다', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoZP-7YRHg54",
        "outputId": "888ee9f9-4856-49ff-fd57-f43fd484371a"
      },
      "source": [
        "# 형태소와 품사 태그 추출\r\n",
        "pos = kkma.pos(text)   ## text변수에 저장된 문장을 품사 태깅\r\n",
        "print(pos)        ## 태깅된 품사와 형태소들은 리스트 형태로 반환"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('인공지능', 'NNG'), ('챗봇', 'UN'), ('이루', 'NNG'), ('달', 'VV'), ('는', 'ETD'), ('연애', 'NNG'), ('의', 'JKG'), ('과학', 'NNG'), ('에서', 'JKM'), ('학습', 'NNG'), ('하', 'XSV'), ('었', 'EPT'), ('습니다', 'EFN'), ('.', 'SF')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P54avysbIk1e",
        "outputId": "8a708298-972c-4040-99f4-d08b2dcbc284"
      },
      "source": [
        "# 명사만 추출\r\n",
        "nouns = kkma.nouns(text)  \r\n",
        "print(nouns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['인공', '인공지능', '지능', '챗봇', '이루', '연애', '과학', '학습']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRsVtrVuIk-o",
        "outputId": "80328c5a-3bb5-45db-c31e-c0d1828007be"
      },
      "source": [
        "# 문장 구분(분리)\r\n",
        "sentences = \"국내 코스피가 3000선을 돌파하였습니다! 이번 해는 정말 기대가 되네요!\"\r\n",
        "s = kkma.sentences(sentences)   ## 복합 문장(2개 이상의 문장)이 있을 때 문장 단위로 토크나이징\r\n",
        "print(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['국내 코스 피가 3000 선을 돌파하였습니다!', '이번 해는 정말 기대가 되네요!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x61Ey1HIjtuX"
      },
      "source": [
        "# Kkma는 다른 형태소 분석기에 비해 분석 시간은 느리지만, 품사 태그가 다양하다는 장점이 있음."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U3BHCShj3HW"
      },
      "source": [
        "## (2)  Komoran"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrKPYlcNkKK4"
      },
      "source": [
        "# Komoran은 코모란이라고 불리며, 공백이 포함된 형태소 단위로도 분석이 가능하다는 특징이 있다. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWBTIdYhIlF-"
      },
      "source": [
        "from konlpy.tag import Komoran\r\n",
        "# konlpy.tag 패키지의 Komoran 모듈을 불러온다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WobIKsYDkTMg"
      },
      "source": [
        "# Komoran에서 사용하는 3가지 함수\r\n",
        "## morphs(phrase) -> 형태소 단위로 토크나이징 / 리스트 형태로 반환\r\n",
        "## nouns(phrase) -> 품사가 명사인 토큰들만 추출\r\n",
        "## pos(phrase, flatten = True) -> 형태소를 추출한 후 품사 태깅 / 튜플{}형태로 묶여서 리스트[]로 반환"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsqjDDB3IlX4"
      },
      "source": [
        "# 코모란 형태소 분석기 객체 생성\r\n",
        "komoran = Komoran()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdGbIQ-kPtzl"
      },
      "source": [
        "text = \"거인의 어깨위에 올라서라.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9eUiwRjHhEc",
        "outputId": "d573ae23-073a-453d-8300-337ad1f92198"
      },
      "source": [
        "# 형태소 추출\r\n",
        "morphs = komoran.morphs(text)\r\n",
        "print(morphs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['거인', '의', '어깨', '위', '에', '올라서', '라', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUBSDsE_HhK8",
        "outputId": "40a46c57-0f81-4cfc-89e4-70f532b33c6f"
      },
      "source": [
        "# 형태소와 품사 태그 추출\r\n",
        "pos = komoran.pos(text)\r\n",
        "print(pos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('거인', 'NNP'), ('의', 'JKG'), ('어깨', 'NNG'), ('위', 'NNG'), ('에', 'JKB'), ('올라서', 'VV'), ('라', 'EF'), ('.', 'SF')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSr4aCGEHhWO",
        "outputId": "6b36f835-a60f-45e6-b177-71c3c2599d57"
      },
      "source": [
        "# 명사만 추출\r\n",
        "nouns = komoran.nouns(text)\r\n",
        "print(nouns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['거인', '어깨', '위']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUn5YNkNlPE3"
      },
      "source": [
        "# Komoran은 Kkan보다 형태소를 빠르게 분석할 수 있다. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9VRNfGqlaAn"
      },
      "source": [
        "## (3) Okt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnlakVXEHhvU"
      },
      "source": [
        "from konlpy.tag import Okt\r\n",
        "# konlpy.tag 패키지의 Okt 모듈을 불러온다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLqfh5CpluCF"
      },
      "source": [
        "# Okt에서 사용하는 3가지 함수\r\n",
        "## morphs(phrase) -> 형태소 단위 토크나이징 / 리스트 형태로 반환\r\n",
        "## nouns(phrase) -> 품사가 명사인 토큰들만 추출\r\n",
        "## pos(phrase, stem=False, join=False) -> 형태소 추출 후 품사 태깅 / 튜플{}형태로 묶여서 리스트[]로 반환\r\n",
        "## normalize(phrase) -> 정규화 (ex. 사랑햌ㅋㅋㅋ -> 사랑해 ㅋㅋㅋㅋ)\r\n",
        "## phrase(phrase) -> 어구 추출 (ex. [입력]: 오늘 날씨가 좋아요. -> [출력]: '오늘','오늘 날씨','날씨')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNwUeESPQEEu"
      },
      "source": [
        "# Okt 형태소 분석기 객체 생성\r\n",
        "okt = Okt()\r\n",
        "\r\n",
        "text = \"인공지능을 배우는 채널 에이림\"\r\n",
        "text_1 = \"내가 좋아하는 가수 아리아나그란데\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZApCWuMYQESf",
        "outputId": "50d66309-8020-4f14-a17f-f9a62d3feaf8"
      },
      "source": [
        "# 형태소 추출\r\n",
        "morphs = okt.morphs(text)\r\n",
        "print(morphs)\r\n",
        "\r\n",
        "morphs = okt.morphs(text_1)\r\n",
        "print(morphs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['인공', '지능', '을', '배우는', '채널', '에이', '림']\n",
            "['내', '가', '좋아하는', '가수', '아리아나', '그란', '데']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRmTOLvqQEfm",
        "outputId": "c44e36cf-6bf9-4d0b-b991-63fea67b37c7"
      },
      "source": [
        "# 형태소와 품사 태그 추출\r\n",
        "pos = okt.pos(text)\r\n",
        "print(pos)\r\n",
        "\r\n",
        "pos = okt.pos(text_1)\r\n",
        "print(pos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('인공', 'Noun'), ('지능', 'Noun'), ('을', 'Josa'), ('배우는', 'Verb'), ('채널', 'Noun'), ('에이', 'Noun'), ('림', 'Noun')]\n",
            "[('내', 'Noun'), ('가', 'Josa'), ('좋아하는', 'Adjective'), ('가수', 'Noun'), ('아리아나', 'Noun'), ('그란', 'Modifier'), ('데', 'Noun')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aeka9rRNQErY",
        "outputId": "715c8462-ce11-4c3d-90d0-b01ab8923d1e"
      },
      "source": [
        "# 명사만 추출\r\n",
        "nouns = okt.nouns(text)\r\n",
        "print(nouns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['인공', '지능', '채널', '에이', '림']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysfq6-n1QE2-",
        "outputId": "92215dbb-dfb0-4e81-f3ab-319c49cfe0ad"
      },
      "source": [
        "# 정규화\r\n",
        "text_2 = \"롤 한판 하실래욬ㅋㅋㅋㅋ?\"\r\n",
        "normalize = okt.normalize(text_2)\r\n",
        "print(normalize)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "롤 한판 하실래요ㅋㅋㅋ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjDEziSvSeaa",
        "outputId": "6e996491-1e87-48c1-8bba-708cde487717"
      },
      "source": [
        "# 어구 추출\r\n",
        "phrases = okt.phrases(text_2)\r\n",
        "print(phrases)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['롤 한판', '한판']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LCoZjugmuNV"
      },
      "source": [
        "# Okt는 분석되는 품사 정보는 작지만 분석 속도는 제일 빠르다. 또한 normalize()함수로  오타가 섞인 문장을 정규화할 수 있다는 장점이 있다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f-92gRZneM1"
      },
      "source": [
        "# 1.2.1 사용사 사전 구축"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1k_gDIKpTdN"
      },
      "source": [
        "# 새롭게 생겨나는 단어나 문장은 형태소 분석기에서 인식이 안되는 경우가 많다. \r\n",
        "# 이를 해결하기 위해 대부분의 형태소 분석기들은 사용자 사전을 추가할 수 있도록 구성되어 있다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV8Y8XlVS1up",
        "outputId": "417adfb6-d81c-4741-8392-94b4f9fee2b9"
      },
      "source": [
        "# 우리는 Komoran을 선택해서 사용할 것이다.\r\n",
        "from konlpy.tag import Komoran\r\n",
        "\r\n",
        "komoran = Komoran(userdic='/content/drive/MyDrive/6주차')\r\n",
        "text = \"에이림은 최고의 인공지능스터디 플랫폼이야\"\r\n",
        "pos = komoran.pos(text)\r\n",
        "print(pos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('에', 'NNG'), ('이림', 'NNP'), ('은', 'JX'), ('최고', 'NNG'), ('의', 'JKG'), ('인공지능', 'NNP'), ('스터디', 'NNG'), ('플랫폼', 'NNP'), ('이야', 'JX')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n76pNWbnpw3H"
      },
      "source": [
        "# '에이림'이라는 단어를 '에','이림'으로 분리해 인식해버렸다. 단어 토큰을 사용해 의미를 분석하는 챗봇의 경우에는 오류를 낼 확률이 높다.\r\n",
        "# 이를 해결하려면 Komoran의 사용자 사전에 '에이림'이라는 신규 단어를 등록해야한다. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_E0DdXAxS13E",
        "outputId": "c7c924f7-c7a4-4303-f99b-1379885f7493"
      },
      "source": [
        "from konlpy.tag import Komoran\r\n",
        "\r\n",
        "komoran = Komoran()\r\n",
        "text = \"올해 모두 건강하시고 행복만 가득하길 바라요\"\r\n",
        "\r\n",
        "pos = komoran.pos(text)\r\n",
        "print(pos)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('올해', 'NNG'), ('모두', 'MAG'), ('건강', 'NNG'), ('하', 'XSV'), ('시', 'EP'), ('고', 'EC'), ('행복', 'NNG'), ('만', 'JX'), ('가득', 'MAG'), ('하', 'XSV'), ('기', 'ETN'), ('ㄹ', 'JKO'), ('바라', 'VV'), ('요', 'EC')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtXAkIiHS1_v",
        "outputId": "78bad15a-ea48-4b6e-e848-7d5a74ae8706"
      },
      "source": [
        "# 메모장을 열어 단어와 품사를 직접 등록해보자. 구분은 반드시 Tab으로 해야한다. \r\n",
        "komoran = Komoran(userdic='/content/drive/MyDrive/6주차/user_dic.tsv')\r\n",
        "text = \"올해 모두 건강하시고 행복만 가득하길 바라요\"\r\n",
        "\r\n",
        "pos = komoran.pos(text)\r\n",
        "print(pos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('올해', 'NNG'), ('모두', 'MAG'), ('건강', 'NNG'), ('하', 'XSV'), ('시', 'EP'), ('고', 'EC'), ('행복', 'NNG'), ('만', 'JX'), ('가득', 'MAG'), ('하', 'XSV'), ('기', 'ETN'), ('ㄹ', 'JKO'), ('바라', 'VV'), ('요', 'EC')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0DmXjIoqlTP"
      },
      "source": [
        "# 지금까지 한국어 토크나이징을 알아보았다. 한국어의 경우 명사와 조사를 띄어쓰지 않고, 용언에 따라 여러 가지 어미가 붙기 때문에 띄어쓰기만으로는 토크나이징을 할 수 없다.\r\n",
        "# 따라서 KoNLPy의 형태소 분석기를 이용해 형태소 단위의 토큰과 품사 정보까지 추출하였다. \r\n",
        "# 그리고 추출된 정보에서 필요 없는 정보를 제거하는 과정이 추가되어야 한다. 이를 '전처리'라고 한다.\r\n",
        "# 전처리 과정은 챗봇 엔진에서 문장의 의미나 의도를 빠르게 잘 분석하기 위해 필수적으로 거쳐야 하는 작업이다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFiSV016n5YY"
      },
      "source": [
        "# 2.1.1 임베딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJKc6uGmrKPt"
      },
      "source": [
        "# 컴퓨터는 수치 연산만 가능하기 때문에 자연어를 숫자나 벡터 형태로 변환할 필요가 있다. 이를 '임베딩'이라고 한다.\r\n",
        "# 즉, 임베딩은 단어나 문장을 수치화해 벡터 공간으로 표현하는 과정을 의미한다.\r\n",
        "# 이러한 임베딩은 말뭉치의 의미에 따라 벡터화하기 때문에 문법적인 정보가 포함되어 있다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZZofzDLrjqu"
      },
      "source": [
        "# 임베딩 기법에는 '문장 임베딩'과 '단어 임베딩'이 있다.\r\n",
        "# '문장 임베딩'은 문장 전체를 벡터로 표현하는 방법이며, '단어 임베딩'은 개별 단어를 벡터로 표현하는 방법이다.\r\n",
        "# '문장 임베딩'의 경우 전체 문장의 흐름을 파악해 벡터로 변환하기 때문에 문맥적 의미를 지니는 장점이 있다.\r\n",
        "# 그러한 이유로 단어 임베딩에 비해 품질이 좋으며 상용 시스템이 많이 사용된다. 하지만 많은 문장 데이터가 필요하며 학습하는데 비용이 많이 들어간다.\r\n",
        "# '단어 임베딩'은 동음이의어에 대한 구분을 하지 않기 때문에 의미가 다르더라도 단어의 형태가 같다면 동일한 벡터값으로 표현되는 단점이 있다.\r\n",
        "# 하지만 학습 방법이 간단하고 컴퓨터에서 처리하기에 효과적이라는 장점이 있다. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBX0LZbZoLyz"
      },
      "source": [
        "# 2.1.2 원-핫 인코딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rFRYGdDsdkC"
      },
      "source": [
        "# 원-핫 인코딩은 단어를 숫자 벡터로 변환하는 가장 기본적인 방법이다.\r\n",
        "# 요소들 중 단 하나의 값만 1이고, 나머지 요솟값은 0인 인코딩을 의미한다.\r\n",
        "# 원-핫 인코딩으로 나온 결과를 원-핫 벡터라고 하며, 전체 요소 중 단 하나의 값만 1이기 때문에 '희소벡터'라고도 한다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSHbXLrQsvn5"
      },
      "source": [
        "# 원-핫 인코딩을 하기 위해서는 단어 집합이라 불리는 사전을 먼저 만들어야 한다.\r\n",
        "# 여기서 사전은 말뭉치에서 나오는 서로 다른 모든 단어의 집합을 의미한다. 말뭉치에 존재하는 모든 단어의 수가 원-핫 벡터의 차원을 결정한다.\r\n",
        "# 예를 들어 100개의 단어가 존재한다면 원-핫 벡터의 크기는 100차원이 된다. 사전이 구축되었다면 사전 내 단어 순서대로 고유한 인덱스 번호가 부여된다.\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlLiVZQGS2IS"
      },
      "source": [
        "# pip install konlpy\r\n",
        "from konlpy.tag import Komoran"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1TzUgdpSef7"
      },
      "source": [
        "import numpy as np\r\n",
        "komoran = Komoran(userdic ='/content/drive/MyDrive/6주차/user_dic.tsv')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKAr73YdNL8P"
      },
      "source": [
        "text = \"내가 좋아하는 커피는 라테야. 하지만 오늘은 아메리카노를 마실래\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6wyLfj-SetC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c487234-453d-49d9-e3b0-efd84bdf6849"
      },
      "source": [
        "# 명사만 추출\r\n",
        "nouns = komoran.nouns(text)\r\n",
        "print(nouns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['커피', '라테', '오늘', '아메리카노']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nFL6IyQQFD6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16385598-4b89-45d9-90dc-0d233fbe7159"
      },
      "source": [
        "# 단어 사전 구축 및 단어별 인덱스 부여\r\n",
        "dics = {}   ## 빈 사전 형식을 구축합니다.\r\n",
        "for word in nouns:    \r\n",
        "## 앞서 설정한 nouns의 값을 하나씩 받아오기 위해 반복문을 활용합니다.\r\n",
        "  if word not in dics.keys():\r\n",
        "## 이 과정은 사전에 같은 단어가 반복적으로 들어오는 것을 방지하기 위한 조건문입니다. \r\n",
        "## ※ 'not in' 확인하기\r\n",
        "    dics[word] = len(dics)\r\n",
        "## 앞서 설정한 dics를 활용하여 사전(dics)의 [word]로 할당하여 줍니다.\r\n",
        "\r\n",
        "print(dics)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'커피': 0, '라테': 1, '오늘': 2, '아메리카노': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGDqmhSYHiNA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a3faaad-53f6-4d26-cf72-a4abbcf842d7"
      },
      "source": [
        "# 원-핫 인코딩\r\n",
        "nb_classes      = len(dics)   # 원-핫 벡터 차원의 크기를 결정한다. 단어 사전은 크기가 원-핫 벡터의 크기이다.\r\n",
        "print(nb_classes)\r\n",
        "targets         = list(dics.values())\r\n",
        "print(targets)\r\n",
        "one_hot_targets = np.eye(nb_classes)[targets] # 원 - 핫 벡터를 만들기 위해서는 넘파이의 eye()함수를 이용할 수 있다.\r\n",
        "# eye()함수는 단위행렬을 만들어 준다. eye()함수의 인자 크기대로 단위행렬을 반환하며, eye()함수 뒤에 붙은 [targets]을 이용해 생성된 단위행렬의 순서를 단어 사전의 순서에 맞게 정렬해준다.\r\n",
        "print(one_hot_targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "[0, 1, 2, 3]\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pqdaH9duDqT"
      },
      "source": [
        "# 원-핫 인코딩의 경우 간단한 구현 방법에 비해 좋은 성능을 가지기 때문에 많은 사람들이 사용하고 있다.\r\n",
        "# 하지만 단순히 단어의 순서에 의한 인덱스값을 기반으로 인코딩된 값이기 때문에 단어의 의미나 유사한 단어와의 관계를 담고 있지 않다. 즉, 비선형적이다.\r\n",
        "# 또한 단어 사전의 크기가 커짐에 따라 원-핫 벡터의 차원도 커지는데 이때 메모리 낭비와 계산의 복잡도가 커지고, 대부분의 요소가 0의 값을 가지고 있으므로 비효율적이다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqmtCSbnoWWP"
      },
      "source": [
        "## 2.1.3 희소 표현과 분산 표현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvKntfZFwHt7"
      },
      "source": [
        "### (1) 희소 표현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1Q2AIRkunf8"
      },
      "source": [
        "# 앞서 소개한 원-핫 인코딩은 표현하고자 하는 단어의 인덱스 요소만 1이고 나머지 요소는 모두 0으로 표현되는 희소벡터(또는 희소행렬)이다. \r\n",
        "# 이처럼 단어가 희소 벡터로 표현되는 방식을 희소표현이라고 부른다.\r\n",
        "# '희소 표현'은 각각의 차원이 독립적인 정보를 지니고 있어 사람이 이해하기에 직관적인 장점이 있지만, 단어 사전의 크기가 커질수록 메모리 낭비와 계산 복잡도가 커지는 단점도 있다.\r\n",
        "# 또한 단어 간의 연관성이 전혀 없어 의미를 담을 수 없다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzT8B5qQwLSD"
      },
      "source": [
        "### (2) 분산 표현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNTyxotwvFD4"
      },
      "source": [
        "# 자연어 처리를 잘 하기 위해서는 '기본 토큰이 되는 단어의 의미'와 '주변 단어 간의 관계'가 단어 임베딩에 표현되어야 한다. 하지만 희소표현의 경우 그러지 못한다.\r\n",
        "# 이를 해결하기 위해 각 단어 간의 유사성을 잘 표현하면서도 벡터 공간을 절약할 수 있는 방법을 고안했는데 이를 '분산 표현'이라고 한다.\r\n",
        "# '분산 표현'은 한 단어의 정보가 특정 차원에 표현되지 않고 여러 차원에 분산되어 표현된다하여 붙여진 이름이다. 즉, 하나의 차원에 다양한 정보를 가지고 있다.\r\n",
        "# 대표적으로 색상을 표현하는 RGB모델은 3차원 형태의 벡터로 생각할 수 있으며, 분산 표현 방식이다. 예를 들어 연두색은 RGB(204, 255, 204)형태로 분산표현을 하는 것이다.\r\n",
        "# 이를 0과 1으로만 이루어진 희소 표현으로 한다면 벡터 차원의 크기는 엄청날 것이다. 뿐만 아니라 다른 색상과의 유사성도 파악할 수 없을 것이다.\r\n",
        "# 이처럼 분산표현은 희소표현에 비해 장점이 많아 단어 임베딩 기법에서 많이 사용되고 있는 방식이다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8B8hBCpwSkb"
      },
      "source": [
        "# 이처럼 분산 표현 방식을 사용하면 단어 임베딩 벡터가 더 이상 희소하지 않다. 신경망에서는 분산 표현을 학습하는 과정에서 임베딩 벡터의 모든 차원에 의미 있는 데이터를 고르게 밀집시킨다. \r\n",
        "# 이 때문에 희소 표현과 반대로 데이터 손실을 최소화하면서 벡터 차원이 압축되는 효과가 생긴다. \r\n",
        "# 분산 표현은 우리가 원하는 차원에 데이터를 최대한 밀집시킬 수 있어 밀집 표현이라 부르기도 하며, 밀집 표현으로 만들어진 벡터를 '밀집 벡터'라 한다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJTRKAHuwxio"
      },
      "source": [
        "# 분산 표현의 장점\r\n",
        "# 1. 임베딩 벡터의 차원을 데이터 손실을 최소화하면서 압축할 수 있다. Cf) '희소 표현' => 입력 데이터의 차원이 너무 높아지면 신경망 모델의 학습이 어려워지는 '차원의 저주' 문제가 발생\r\n",
        "# 2. 단어의 의미, 주변 단어간의 관계 등 많은 정보가 표현되어 있어 일반화 능력이 뛰어나다. => ex) '여자' - '여성'을 구별할 수 있다.\r\n",
        "## 벡터 공간 상에서 유사한 의미를 갖는 단어들은 비슷한 위치에 분포되어 있기 때문에 '여자'와 '여성'의 단어 위치는 매우 가깝다. \r\n",
        "## 이런 두 단어 간의 거리를 계산할 수 있으면 컴퓨터는 '여자'와 '여성' 두 단어를 같은 의미로 해석할 수 있다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3PaH4YibyPh"
      },
      "source": [
        "# 3.1.1 Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmCWere6ygjg"
      },
      "source": [
        "# 신경망 기반 단어 임베딩의 대표적인 방법 -> Word2Vec\r\n",
        "## Word2Vec은 2013년에 구글에서 발표했으며 가장 많이 사용하고 있는 단어 임베딩 모델이다.\r\n",
        "## 이 모델은 CBOW(continuous bag - of words)와 skip-gram 두 가지 모델로 제안되었다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpnCKkVAy-na"
      },
      "source": [
        "# CBOW 모델\r\n",
        "## CBOW모델은 맥락(context word)이라 표현되는 주변 단어들을 이용해 타겟 단어를 예측하는 신경망 모델이다.\r\n",
        "## 신경망의 입력을 주변 단어들로 구성하고, 출력을 타깃 단어로 설정해 학습된 가중치 데이터를 임베딩 벡터로 활용한다.\r\n",
        "## CBOW 모델에서는 타깃 단어를 예측하기 위해 앞뒤 단어를 확인했다. 이 때 앞뒤로 몇 개의 단어까지 확인할지 결정할 수 있는데, 이 범위를 윈도우(window)라고 한다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFm_J-hgzfrw"
      },
      "source": [
        "# skip-gram 모델\r\n",
        "## 하나의 타깃 단어를 이용해 주변 단어들을 예측하는 신경망 모델이다.\r\n",
        "## 입출력이 CBOW모델과 반대로 되어있기 때문에 skip-gram모델이 CBOW 모델에 비해 예측해야 하는 맥락이 많아진다.\r\n",
        "## 따라서 단어 분산 표현력이 우수해 CBOW 모델에 비해 임베딩 품질이 우수하다.\r\n",
        "## 반면, CBOW 모델은 타깃 단어의 손실만 계산하면 되기 때문에 학습 속도가 빠른 장점이 있다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhkD4G7J0lLH"
      },
      "source": [
        "# Word2Vec의 단어 임베딩은 해당 단어를 밀집 벡터로 표현하며 학습을 통해 의미상 비슷한 단어들을 비슷한 벡터 공간에 위치시킨다.\r\n",
        "# 또한 벡터 특성상 의미에 따라 방향성을 갖게된다. 임베딩된 벡터들 간 연산이 가능하기 때문에 단어간 관계를 계산할 수 있다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-GbTCWWO5-_",
        "outputId": "4ab50959-297e-4b13-ff61-c0bba3b237fc"
      },
      "source": [
        "pip install KoNLPy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: KoNLPy in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from KoNLPy) (1.19.5)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from KoNLPy) (0.4.4)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from KoNLPy) (4.2.6)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from KoNLPy) (4.6.0)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from KoNLPy) (3.10.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from KoNLPy) (1.2.1)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->KoNLPy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->KoNLPy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->KoNLPy) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->KoNLPy) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->KoNLPy) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->KoNLPy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->KoNLPy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->KoNLPy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->KoNLPy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->KoNLPy) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VIxBSgVcQJp"
      },
      "source": [
        "from gensim.models import Word2Vec\r\n",
        "from konlpy.tag import Komoran\r\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d5OJwjJ1WuX"
      },
      "source": [
        "# 한국어 Word2Vec를 만들기 위해서는 한국어 말뭉치를 수집해야 한다. 네이버 영화 리뷰를 이용해 Word2Vec 모델을 만들어보자."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs9PMUh-cQR1"
      },
      "source": [
        "# 네이버 영화 리뷰 데이터 읽어옴\r\n",
        "def read_review_data(filename):     # read_review_data()함수는 리뷰 데이터를 각 라인 별로 읽어와 \\t를 기준으로 데이터를 분리한다. 그 후 첫 번째 행의 헤더를 제거하고 리뷰 데이터만 반환한다.\r\n",
        "  with open(filename, 'r') as f:\r\n",
        "    data = [line.split('\\t') for line in f.read().splitlines()]\r\n",
        "    data = data[1:]   # header 제거\r\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI-5GMvecRJU",
        "outputId": "a9d9327c-0d98-4f43-ece4-25da9698e15e"
      },
      "source": [
        "# 현재 시간 측정 시작\r\n",
        "start = time.time()\r\n",
        "\r\n",
        "# 리뷰 파일 읽어오기\r\n",
        "print('1) 말뭉치 데이터 읽기 시작')\r\n",
        "review_data = read_review_data('/content/drive/MyDrive/6주차/ratings.txt')  # read_review_data()함수를 호출해 현재 경로에 있는 ratings.txt파일을 리스트 형태로 읽어온다.\r\n",
        "print(len(review_data))  # 리뷰 데이터 전체 개수\r\n",
        "print('1) 말뭉치 데이터 읽기 완료: ', time.time() - start) # 데이터 읽는데 소요되는 시간 측정"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1) 말뭉치 데이터 읽기 시작\n",
            "200000\n",
            "1) 말뭉치 데이터 읽기 완료:  0.45798158645629883\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UA7MutRcRSH",
        "outputId": "362d813a-4d27-46ad-e0bc-8f1817c6be54"
      },
      "source": [
        "# 문장단위로 명사만 추출해 학습 입력 데이터로 만듬\r\n",
        "print('2) 형태소에서 명사만 추출 시작')\r\n",
        "komoran = Komoran()     # Komoran 형태소 분석기를 이용해 불러온 리뷰 데이터에서 문장별로 명사만 추출한다. \r\n",
        "docs = [komoran.nouns(sentence[1]) for sentence in review_data]     #  sentence[1]은 rating.txt파일에서 document 컬럼의 데이터를 의미한다. \r\n",
        "print('2) 형태소에서 명사만 추출 완료:', time.time() - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2) 형태소에서 명사만 추출 시작\n",
            "2) 형태소에서 명사만 추출 완료: 6151.034969568253\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_1PiWy_cReq",
        "outputId": "350ba3ca-d2f3-4085-a988-5a5f2a6cf971"
      },
      "source": [
        "# word2vec 모델 학습\r\n",
        "print('3) word2vec 모델 학습 시작')             # sentences = Word2Vec 모델 학습에 필요한 문장 데이터(입력값) / size = 단어 임베딩 벡터의 차원(크기)\r\n",
        "model = Word2Vec(sentences=docs, size =200, window =4, min_count=2, sg=1)    # window = 주변 단어 윈도우 크기 /  hs = 0(0이 아닌 경우 음수 샘플링 사용, 1(모델 학습에 softmax사용)\r\n",
        "print('3) word2vec 모델 학습 완료: ', time.time() - start)     #  min_count = 단어 최소 빈도 수 제한 / sg = 0(CBOW 모델), 1(skip-gram 모델)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3) word2vec 모델 학습 시작\n",
            "3) word2vec 모델 학습 완료:  6211.852521181107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3vJCM9acRmT",
        "outputId": "80917929-8241-4007-f157-bdff1055e1d0"
      },
      "source": [
        "# 모델 저장\r\n",
        "print('4) 학습된 모델 저장 시작')\r\n",
        "model.save('/content/nvmc.model')\r\n",
        "print('4) 학습된 모델 저장 완료: ', time.time() - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4) 학습된 모델 저장 시작\n",
            "4) 학습된 모델 저장 완료:  8275.546051502228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdBSdPzE3Iv-",
        "outputId": "c90d7ff2-839d-44b9-ed1e-dfa582a8a8b0"
      },
      "source": [
        "# import os\r\n",
        "# print(os.getcwd())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/6주차\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVcdiFVK3J5q"
      },
      "source": [
        "# os.chdir('/content/drive/MyDrive/6주차')\r\n",
        "#   # change+dir = 경로변경"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZDRloOj3KBU",
        "outputId": "4ae5e77e-ef05-4a3c-c35d-ed6a26026150"
      },
      "source": [
        "# print(os.getcwd())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/6주차\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLY1kpSq-uH4",
        "outputId": "a999f8f0-4f31-43a5-d798-537001283e8e"
      },
      "source": [
        "# 학습된 말뭉치 개수, 말뭉치 내 전체 단어 개수\r\n",
        "print(\"corpus_count : \", model.corpus_count)   # corpus = 말뭉치\r\n",
        "print(\"corpus_total_words : \", model.corpus_total_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus_count :  200000\n",
            "corpus_total_words :  1076896\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cczggwPH7v9O",
        "outputId": "9af83553-64e7-4584-dd53-00572dd102bd"
      },
      "source": [
        "# 이제 생성된 Word2Vec 모델 파일(nvmc.model)을 읽어와 실제로 단어 임베딩된 값과 벡터 공간상의 유사한 단어들을 확인한다.\r\n",
        "from gensim.models import Word2Vec\r\n",
        "# 모델 로딩\r\n",
        "model = Word2Vec.load('nvmc.model')  ## 앞서 생성한 모델 파일을 읽어와 Word2Vec 객체를 생성한다.\r\n",
        "print(\"corpus_total_words : \", model.corpus_total_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus_total_words :  1076896\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mf9KzCzCAJoL",
        "outputId": "632bb59a-a960-419c-8ea2-9341c7d46e54"
      },
      "source": [
        "# '사랑'이란 단어로 생성한 단어 임베딩 벡터\r\n",
        "print('사랑 : ', model.wv['사랑'])\r\n",
        "## 아까 size=200 설정해서 200개가 나옴"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "사랑 :  [ 0.01674293  0.51467186 -0.07588267  0.5123789  -0.01060495  0.07019342\n",
            "  0.12742509  0.43002895  0.04758167 -0.30151933  0.05330271 -0.38831586\n",
            "  0.07432814 -0.07065358  0.25120553 -0.16846009 -0.0024971   0.2222305\n",
            "  0.03458407  0.25135908 -0.3866345  -0.09421609  0.11133826 -0.05075794\n",
            " -0.2874938   0.42816207  0.04822439  0.431476    0.01963828  0.19921312\n",
            "  0.20904988  0.19159354 -0.14315651 -0.29762885  0.516501    0.17777888\n",
            " -0.070075   -0.31060907  0.00287258 -0.09658547  0.09623053 -0.12714906\n",
            "  0.09824736  0.13891724  0.45129165 -0.07572363 -0.24889077  0.18211159\n",
            " -0.1045812  -0.13736753  0.12836139  0.06364421  0.19511682  0.37865475\n",
            " -0.13012664 -0.2883566   0.28840566  0.11584891  0.18054256  0.1680498\n",
            " -0.3812222  -0.10812126 -0.04049836  0.29437268  0.04748166  0.08426711\n",
            " -0.06646287 -0.29517213  0.2042632   0.24938314  0.07123461  0.0746418\n",
            "  0.02084113 -0.03910499 -0.18127085 -0.10155111  0.06480198  0.0263683\n",
            " -0.02088292 -0.01011798  0.09046382  0.4934662   0.19957504  0.27350044\n",
            " -0.11162363 -0.18224382  0.29755262 -0.20146853  0.17488055 -0.33087805\n",
            " -0.10502163  0.13570076  0.1230625  -0.2954163   0.07353892 -0.22101158\n",
            " -0.07469536 -0.03132002  0.00338453  0.29505572 -0.28543377 -0.1289448\n",
            "  0.28569686  0.14448509  0.26207048  0.27125916 -0.17038332 -0.10099693\n",
            " -0.12493234 -0.33872512  0.6331383  -0.01091467 -0.06622886 -0.35856807\n",
            " -0.4181587   0.21780589  0.04245605 -0.15021808 -0.09977298  0.54205334\n",
            " -0.07127061 -0.08058133 -0.07716531  0.18926057  0.00286955  0.07613236\n",
            "  0.2356907   0.02881617 -0.28714022  0.59816885  0.10830243 -0.04851647\n",
            "  0.14369257  0.24315116 -0.33998966 -0.06731903  0.07256666  0.40133366\n",
            "  0.13755254 -0.3271864   0.28237268 -0.2967971  -0.47906813  0.3278635\n",
            " -0.18771899 -0.4454305  -0.16080567 -0.01648401 -0.07909369  0.28894135\n",
            "  0.19151592 -0.03723706 -0.12117841 -0.03502955 -0.47134483 -0.0445674\n",
            " -0.36470735 -0.28602663 -0.21334106 -0.354214    0.23933244  0.32187584\n",
            " -0.24849936 -0.17096321 -0.03502386 -0.16986044  0.07069104 -0.42212078\n",
            " -0.22660045  0.15553264 -0.4167079   0.02636648 -0.19975437  0.49317926\n",
            " -0.33055422 -0.07937501  0.33897206  0.11381218 -0.05589493  0.18028964\n",
            "  0.18021835  0.0496711   0.17503378 -0.20244843 -0.02050086  0.062589\n",
            " -0.10068191 -0.4726133   0.19521722 -0.06127131 -0.14898017  0.5001003\n",
            " -0.16255358  0.08508348  0.12366963 -0.34542853  0.43985555 -0.6308256\n",
            "  0.22970247  0.01941524]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbpvmXG33KYu",
        "outputId": "f911f28f-3f5c-4991-c579-9a9020c32d33"
      },
      "source": [
        "# 가장 유사한 단어 추출\r\n",
        "from gensim.models import Word2Vec\r\n",
        "print(model.wv.most_similar(\"반지의 제왕\", topn=5)) \r\n",
        "## gensim 패키지의 model.wv.most_similarity()함수를 호출할 경우 인자로 사용한 단어와 가장 유사한 단어를 리스트로 반환해준다.\r\n",
        "## topn인자는 반환되는 유사한 단어 수를 의미한다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('에이리언', 0.9522925615310669), ('엑스맨', 0.9510397911071777), ('오리지널', 0.9443783760070801), ('리부트', 0.9424226880073547), ('능가', 0.9418059587478638)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_C_Bl1nX8yRF",
        "outputId": "1564fd1f-7935-4647-a652-2dd2148bc36d"
      },
      "source": [
        "# 단어 유사도 계산 (1로 가까이 갈 수록 유사도가 높아짐)\r\n",
        "print(model.wv.similarity(w1=\"스타워즈\", w2=\"아버지\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.30098435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4cTLpJa3K6m",
        "outputId": "99195ca3-95d7-4ff3-ea75-30eabbe0d6f3"
      },
      "source": [
        "print(model.wv.similarity(w1=\"삼성\", w2 = \"주식\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.73561066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8gJljS63LEe"
      },
      "source": [
        "# 예제를 통해 실습해보면 놀라울 정도로 유사한 단어를 찾는 경우도 있지만, 이해하기 힘든 결과를 출력하는 경우도 있다.\r\n",
        "# 이는 주제에 맞는 말뭉치 데이터가 부족해서 생기는 현상이니 품질 좋은 말뭉치 데이터를 학습하면 임베딩 성능이 많이 좋아진다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxFKUcFo3LgJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}