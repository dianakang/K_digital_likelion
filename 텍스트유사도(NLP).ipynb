{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "텍스트유사도(NLP).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMl5O4fH3RT98HB2aMDwILc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dianakang/K_digital_likelion/blob/master/%ED%85%8D%EC%8A%A4%ED%8A%B8%EC%9C%A0%EC%82%AC%EB%8F%84(NLP).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_zQfZuKhfqp"
      },
      "source": [
        "# 자연어 처리에서 문장 간에 의미가 얼마나 유사한지 계산하는 일은 매우 중요하다.\r\n",
        "# 문장 역시 단어들의 묶음이기 때문에 하나의 벡터로 묶어서 문장간의 유사도를 계산할 수 있다. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSN8vUeThysS"
      },
      "source": [
        "# 챗봇 개발을 보더라도 챗봇 엔진에 입력되는 문장과 시스템에서 해당 주제의 답변과 연관되어 있는 질문이 얼마나 유사한지 계산할 수 있어야 적절한 답변을 출력할 수 있다. \r\n",
        "# 이처럼 두 문장 간의 유사도를 계산하기 위해서는 문장 내에 존재하는 단어들을 수치화 해야한다. \r\n",
        "# 이때는 언어 모델에 따라 1. 통계를 이용하는 방법과 2. 인공 신경망을 이용하는 방법으로 나눌 수 있다. \r\n",
        "# 앞서 배웠던 Word2Vec은 인공 신경망을 이용했고, 이번에는 통계적인 방법을 이용해 유사도를 계산하는 방법을 살펴볼 것이다. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnZ_a8B8iz1E"
      },
      "source": [
        "# 1. n-gram 유사도"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmnDHD0mi6mc"
      },
      "source": [
        "# n - gram은 주어진 문장에서 n개의 연속적인 단어 시퀀스(단어 나열)을 의미한다. \r\n",
        "# n - gram은 문장에서 n개의 단어를 토큰으로 사용한다. 이는 이웃한 단어의 출현 횟수를 통계적으로 표현해 텍스트의 유사도를 계산하는 방법이다. \r\n",
        "# n이 1인 경우 1-gram 또는 유니그램, 2인 경우 2-gram 바이그램, 3인 경우 3-gram 또는 트라이그램이라 부르며, 4이상인 숫자만 앞쪽에 붙여 부른다.\r\n",
        "# 이제 n-gram을 이용해 문장 간의 유사도를 계산해보자! 문장을 n-gram으로 토큰을 분리한 후, '단어 문서 행렬(Term-Document Matrix, TDM)'을 만든다.\r\n",
        "# 이후 두 문장을 서로 비교해 동일한 단어의 출현 빈도를 확률로 계산해 유사도를 구할 수 있다. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIfhnTj1kdmk"
      },
      "source": [
        "# tf(term frequency)는 두 문장 A와 B에서 동일한 토큰의 출현 빈도를 뜻하며, tokens는 해당 문장에서 전체 토큰 수를 의미한다. \r\n",
        "# 토큰이란 n - gram으로 분리된 단어이다. 즉, 기준이 되는 문장 A에서 나온 전체 토큰 중에서 A와 B에 동일한 토큰이 얼마나 있는지 비율로 표현한 수식이다. \r\n",
        "# 1.0에서 가까울 수록 B가 A에 유사하다고 볼 수 있다. \r\n",
        "# 이제 n-gram의 개념을 정리했으니, 실제로 작동하는 코드를 만들어보자. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iN1bmDOTWJIs",
        "outputId": "93ce3f40-8400-4a84-e215-c3d14e07eb0f"
      },
      "source": [
        "pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNkRj0VCVouz"
      },
      "source": [
        "from konlpy.tag import Komoran"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7dnxgROTKGI",
        "outputId": "66055692-c11a-48bc-c30e-fb01a89947df"
      },
      "source": [
        "# 어절 단위 n-gram\r\n",
        "def word_ngram(bow, num_gram):  # bow = bag of words\r\n",
        "  text = tuple(bow)\r\n",
        "  ngrams = [text[x:x + num_gram] for x in range(0, len(text))]\r\n",
        "  return tuple(ngrams)    ## 추출된 토큰들은 튜플 형태로 반환된다.  이전 예시처럼 슬라이싱을 이용해 문장을 어절 단위로 n개씩 끊어서 토큰을 저장한다. \r\n",
        "\r\n",
        "\r\n",
        "# 음절 n-gram 분석\r\n",
        "def phoneme_ngram(bow, num_gram):  \r\n",
        "  sentence = ' ', join(bow)\r\n",
        "  text = tuple(sentence)\r\n",
        "  slen = len(text)\r\n",
        "  ngrams = [text[x:x + num_gram] for x in range(0, slen)]\r\n",
        "  return ngrams\r\n",
        "\r\n",
        "# 유사도 계산\r\n",
        "def similarity(doc1, doc2):    # doc1의 토큰이 doc2의 토큰과 얼마나 동일한지 횟수를 카운트한다.\r\n",
        "  cnt = 0\r\n",
        "  for token in doc1:\r\n",
        "    if token in doc2:\r\n",
        "      cnt = cnt + 1\r\n",
        "  return cnt / len(doc1)     # 카운트 된 값을 doc1의 전체 토큰 수로 나누면 유사도가 계산된다. 이때 결과가 1.0에 가까워질수록 doc1과 유사해진다.\r\n",
        "\r\n",
        "sentence1 = '나는 추운 겨울 1월에 한국에서 태어났다.'\r\n",
        "sentence2 = '나는 추운 겨울 현재 한국에서 지내고 있다'\r\n",
        "sentence3 = '나는 한국을 떠나 해외에서 살고 싶다'\r\n",
        "\r\n",
        "# 형태소 분석기에서 명사(단어)추춯\r\n",
        "komoran = Komoran()\r\n",
        "bow1 = komoran.nouns(sentence1)\r\n",
        "bow2 = komoran.nouns(sentence2)\r\n",
        "bow3 = komoran.nouns(sentence3)\r\n",
        "\r\n",
        "# 단어 n-gram 토큰 추출\r\n",
        "doc1 = word_ngram(bow1, 2)   # 2-gram 방식으로 추출\r\n",
        "doc2 = word_ngram(bow2, 2)\r\n",
        "doc3 = word_ngram(bow3, 2)\r\n",
        "\r\n",
        "# 추출된 n-gram 토큰 출력\r\n",
        "print(doc1)\r\n",
        "print(doc2)\r\n",
        "print(doc3)\r\n",
        "\r\n",
        "# 유사도 계산\r\n",
        "r1 = similarity(doc1, doc2)\r\n",
        "r2 = similarity(doc3, doc1)\r\n",
        "\r\n",
        "# 계산된 유사도 출력\r\n",
        "print(r1)\r\n",
        "print(r2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(('겨울', '1월'), ('1월', '한국'), ('한국',))\n",
            "(('겨울', '한국'), ('한국',))\n",
            "(('한국', '해외'), ('해외',))\n",
            "0.3333333333333333\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE3LyH9QwIzj"
      },
      "source": [
        "# n - gram은 문장에 존재하는 모든 단어의 출현빈도를 확인하는 것이 아니라 연속되는 문장에서 일부 단어(n으로 설정된 개수만큼)만 확인하다 보니\r\n",
        "# 전체 문장을 고려한 언어 모델보다 정확도가 떨어질 수 있다.\r\n",
        "# n을 크게 잡을수록 비교 문장의 토큰과 비교할 때 카운트를 놓칠 확률이 커진다.\r\n",
        "# 하지만 n을 작게 잡을수록 카운트 확률은 높아지지만 문맥을 파악하는 정확도는 떨어질 수 밖에 없으므로 n-gram에서 n의 설정은 매우 중요하다.(보통 1 ~ 5 사이의 값을 많이 사용한다.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57ibplHfk_2q"
      },
      "source": [
        "# 2.코사인 유사도"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzAGZsTww7WT"
      },
      "source": [
        "# 단어나 문장을 벡터로 표현할 수 있다면 벡터 간 거리나 각도를 이용해 유사성을 파악할 수 있다.\r\n",
        "# 벡터 간 거리를 구하는 방법은 다양하지만 우리는 '코사인 유사도'를 설명하겠다. 코사인 유사도는 코사인 각도를 이용해 유사도를 측정하는 방법이다.\r\n",
        "# 일반적으로 코사인 유사도는 벡터의 크기가 중요하지 않을 때 그 거리를 측정하기 위해 사용된다.\r\n",
        "# 예를 들어 단어의 출현 빈도를 통해 유사도 계산을 한다면 동일한 단어가 많이 포함되어 있을 수록 벡터의 크기가 커진다.\r\n",
        "# 이 때 코사인 유사도는 벡터의 크기와 상관없이 결과가 안정적이다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97H8xtmTy47R"
      },
      "source": [
        "# 코사인 값은 -1 ~ 1 사이의 값을 가지며, 두 벡터의 방향이 완전히 동일한 경우에는 1, 반대 방향인 경우에는 -1, 두 벡터가 서로 직각을 이루면 0의 값을 가진다.\r\n",
        "# 즉, 두 벡터의 방향이 같아질 수록 유사하다 볼 수 있다.  우리는 공간 벡터의 내적과 크기를 이용해 코사인 각도를 계산한다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dco9o-QFidZs"
      },
      "source": [
        "from konlpy.tag import Komoran\r\n",
        "import numpy as np\r\n",
        "from numpy import dot  # dot = 벡터 곱(A*B)\r\n",
        "from numpy.linalg import norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etKcUOdHlB-m"
      },
      "source": [
        "# 코사인 유사도 계산\r\n",
        "def cos_sim(vec1, vec2): \r\n",
        "  return dot(vec1, vec2) / (norm(vec1) * norm(vec2))  # 코사인 유사도 계산에는 넘파이에서 제공하는 벡터 내적을 계산하는 함수(vec())와 노름(벡터의 크기)(norm())을 계산하는 함수를 이용한다.\r\n",
        "                                                      # 벡터의 노름에는 여러가지 종류가 있지만, 코사인 유사도에서는 L2노름(유클리드 노름)을 주로 사용한다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx4xggmxi1mP"
      },
      "source": [
        "# TDM(단어문서) 만들기\r\n",
        "def make_term_doc_mat(sentence_bow, word_dics):  # 비교 문장에서 추출한 단어 사전을 기준으로 문장에 해당 단어들이 얼마나 포함되어 있는지 나타내는 단어 문서 행렬 TDM을 만들어주는 함수이다. \r\n",
        "  freq_mat = {}\r\n",
        "\r\n",
        "  for word in word_dics:\r\n",
        "    freq_mat[word] = 0\r\n",
        "  \r\n",
        "  for word in word_dics:\r\n",
        "    if word in sentence_bow:\r\n",
        "      freq_mat[word] += 1\r\n",
        "  return freq_mat\r\n",
        "\r\n",
        "# 단어 벡터 만들기\r\n",
        "def make_vector(tdm):  # 단어 문서 행렬에서 표현된 토큰들의 출현 빈도 데이터를 벡터로 만들어주는 함수이다. \r\n",
        "  vec = []\r\n",
        "  for key in tdm:\r\n",
        "    vec.append(tdm[key])\r\n",
        "  return vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFysKmOqi18Q"
      },
      "source": [
        "# 문장 정의\r\n",
        "sentence1 = '나는 추운 겨울 1월에 한국에서 태어났다.'\r\n",
        "sentence2 = '나는 추운 겨울 현재 한국에서 지내고 있다'\r\n",
        "sentence3 = '나는 한국을 떠나 해외에서 살고 싶다'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqmJp-WWnjb0"
      },
      "source": [
        "# 형태소분석기를 이용해 단어 묶음 리스트 생성\r\n",
        "komoran = Komoran()\r\n",
        "bow1 = komoran.nouns(sentence1)\r\n",
        "bow2 = komoran.nouns(sentence2)\r\n",
        "bow3 = komoran.nouns(sentence3)\r\n",
        "\r\n",
        "## Komoran 형태소 분석기를 이용해 정의된 문장에서 명사를 리스트 형태로 추출한다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZyrl2INni5o"
      },
      "source": [
        "# 단어 묶음 리스트를 하나로 합침\r\n",
        "bow = bow1 + bow2 + bow3   # 3개의 문장에서 추출한 단어 리스트를 하나의 리스트로 합친다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2z9SLiYnnTK"
      },
      "source": [
        "# 단어 묶음에서 중복제거해 단어 사전 구축\r\n",
        "word_dics = []           # 하나로 합쳐진 단어 묶음 리스트에서 중복된 단어를 제거해 새로운 사전 리스트를 구축한다.\r\n",
        "for token in bow:\r\n",
        "  if token not in word_dics:       \r\n",
        "    word_dics.append(token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "own1FwkAUW91",
        "outputId": "05d1eb3c-ec5b-43ce-8717-5908418d4cfb"
      },
      "source": [
        "# 문장 별 단어 문서 행렬 계산\r\n",
        "freq_list1 = make_term_doc_mat(bow1, word_dics)\r\n",
        "freq_list2 = make_term_doc_mat(bow2, word_dics)\r\n",
        "freq_list3 = make_term_doc_mat(bow3, word_dics)\r\n",
        "print(freq_list1)    # 각 문장마다 단어 행렬 리스트를 만든 후 출력한다.\r\n",
        "print(freq_list2)\r\n",
        "print(freq_list3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'겨울': 1, '1월': 1, '한국': 1, '해외': 0}\n",
            "{'겨울': 1, '1월': 0, '한국': 1, '해외': 0}\n",
            "{'겨울': 0, '1월': 0, '한국': 1, '해외': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Js_wcV-kclS"
      },
      "source": [
        "# 문장 벡터 생성\r\n",
        "doc1 = np.array(make_vector(freq_list1))\r\n",
        "doc2 = np.array(make_vector(freq_list2))\r\n",
        "doc3 = np.array(make_vector(freq_list3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDe8C-A4oeoL",
        "outputId": "25dd3c02-da4a-4663-c58b-336b00303d98"
      },
      "source": [
        "# 코사인 유사도 계산\r\n",
        "r1 = cos_sim(doc1, doc2)\r\n",
        "r2 = cos_sim(doc3, doc1)\r\n",
        "print(r1)\r\n",
        "print(r2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8164965809277259\n",
            "0.40824829046386296\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}